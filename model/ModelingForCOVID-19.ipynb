{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Samriddha KC \n",
    "# Based on: https://www.kaggle.com/stoicstatic/twitter-sentiment-analysis-for-beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.6\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/samriddhakc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "DATASET_COLUMNS=[\"sentiment\",\"ids\",\"date\",\"flag\",\"user\",\"text\"]\n",
    "DATASET_ENCODING=\"ISO-8859-1\"\n",
    "\n",
    "\n",
    "dataset=pd.read_csv(\"/Users/samriddhakc/Desktop/training.1600000.processed.noemoticon.csv\", encoding=DATASET_ENCODING,names=DATASET_COLUMNS, engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unecessary columns\n",
    "# replace  4 with 1 for clarity\n",
    "dataset=dataset[[\"sentiment\",\"text\"]]\n",
    "dataset=dataset.replace(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dataset to know the rough distribution of the data. \n",
    "ax=dataset.groupby(\"sentiment\").count().plot(kind='bar',title=\"Data distribution\")\n",
    "ax.set_xticklabels(['Negative',\"Positive\"],rotation=0)\n",
    "# Even distribution of data means less chance for bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store data as a list.\n",
    "sentiments,texts=list(dataset['sentiment']),list(dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Text. \n",
    "# LowerCasing->This erases confusion without the text losing any meaning \n",
    "# For clarity, replace words like http,https,or www by URL \n",
    "# Replacing emojis with words so that it can be used for feature extraction \n",
    "# Replace username with the word USER \n",
    "# Removing non-alphabets\n",
    "# Remove repitive letter >=3 for reducing redudandency \n",
    "# Removing short words->Remove words with less than length 2 because they are mostly irrelevant\n",
    "# Remove stop words:->Does not add much meaning to the whole sentence so it can be ignored. \n",
    "# Lemmantizing=>convert word to its base form to have a concise bag of words and prevent overfitting. \n",
    "\n",
    "emojis={':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "\n",
    "## Defining set containing all stopwords in english.\n",
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
    "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentiments(tweets): \n",
    "    processedData=[]\n",
    "    wordLemm=WordNetLemmatizer()\n",
    "    url_pattern=r\"((https://)[^ ]*|(http://)[^ ]*|(www\\.)[^ ]*)\"\n",
    "    user_pattern=\"@[^\\s]+\"\n",
    "    alpha_pattern=\"[^a-zA-Z0-9]\"\n",
    "    sequence_pattern=r\"(.)\\1\\1+\"\n",
    "    sequence_replace_pattern= r\"\\1\\1\"\n",
    "    for tweet in tweets: \n",
    "        tweet=tweet.lower()\n",
    "        tweet=re.sub(url_pattern,' URL',tweet)\n",
    "        for emoji in emojis.keys(): \n",
    "            tweet=tweet.replace (emoji,\"EMOJ\"+emojis[emoji])\n",
    "        tweet=re.sub(user_pattern,' USER',tweet)\n",
    "        tweet=re.sub(alpha_pattern,\" \",tweet)\n",
    "        tweet=re.sub(sequence_pattern,sequence_replace_pattern,tweet)\n",
    "    \n",
    "        tweet_words=\"\"\n",
    "        for word in tweet.split(' '): \n",
    "            if len(word)>1 and word not in stopwordlist: \n",
    "                word=wordLemm.lemmatize(word)\n",
    "                tweet_words+=(word+' ')\n",
    "        processedData.append(tweet_words)\n",
    "    return processedData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets=preprocess_sentiments(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Cloud for Negative Tweets\n",
    "data_neg=processed_tweets[:800000]\n",
    "plt.figure(figsize=(20,20))\n",
    "wc=WordCloud(max_words=1000,width=1600,height=800,collocations=False).generate(\" \".join(data_neg))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Cloud for Positive Tweets\n",
    "data_pos=processed_tweets[800000:]\n",
    "plt.figure(figsize=(20,20))\n",
    "wc=WordCloud(max_words=1000,width=1600,height=800,collocations=False).generate(\" \".join(data_pos))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting Data into training and test set. \n",
    "X_train,X_test,y_train,y_test=train_test_split(processed_tweets,sentiments,test_size=0.02,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(ngram_range=(1,2),max_features=500000)\n",
    "vectorizer.fit(X_train)\n",
    "print(\"The feature names are\",vectorizer.get_feature_names())\n",
    "print(\"The feature length is\",len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=vectorizer.transform(X_train)\n",
    "X_test=vectorizer.transform(X_test)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model \n",
    "def model_eval(model): \n",
    "    y_pred_train=model.predict(X_train)\n",
    "    print(\"For train data\",classification_report(y_train,y_pred_train))\n",
    "    y_pred=model.predict(X_test)\n",
    "    print(\"For test data\",classification_report(y_test,y_pred))\n",
    "    cf_matrix=confusion_matrix(y_test,y_pred)\n",
    "    categories=['Negative','Positive']\n",
    "    group_names=['True Neg','False Pos','False Neg','True Pos']\n",
    "    group_percentages=['{0:.2%}'.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "    labels=np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cf_matrix,annot=labels,cmap='Blues',fmt='',xticklabels=categories,yticklabels=categories)\n",
    "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model \n",
    "LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
    "LRmodel.fit(X_train, y_train)\n",
    "model_eval(LRmodel)\n",
    "y_pred_train=model.predict(X_train)\n",
    "cf_matrix=confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test rrandom forest model with random tweets. \n",
    "random_texts=[\"I hate you\",\"I love you\",\"Corona virus will kill us\",\"I think trump loves corona virus\",\"I wanna the better situation for my exam, Corona virus is killing people\",\"Corona virus is killing people, please don't let it for the calamity!\",\"Be positive,ignore negativity, this too shall pass!\"]\n",
    "random_refined=preprocess_sentiments(random_texts)\n",
    "X=vectorizer.transform(random_refined)\n",
    "print(X.shape)\n",
    "'''y_pred_train=LRmodel.predict(X)\n",
    "y_pred=LRmodel.predict(X_test)\n",
    "print(\"For test data\",classification_report([1,0,0,1],y_pred))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train=LRmodel.predict(X)\n",
    "print(\"For test data\",classification_report([1,0,0,1],y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_texts_1=[\"I hate you\",\"I love you\",\"Corona virus will kill us\"]\n",
    "random_refined_1=preprocess_sentiments(random_texts_1)\n",
    "X1=vectorizer.transform(random_refined_1)\n",
    "y_pred_train_2=LRmodel.predict(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('ngram.pickle','wb')\n",
    "pickle.dump(vectorizer, file)\n",
    "file.close()\n",
    "\n",
    "file = open('LogisticTrainedLR.pickle','wb')\n",
    "pickle.dump(LRmodel, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(max_depth=100,random_state=0)\n",
    "clf.fit(X_train,y_train)\n",
    "model_eval(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(max_depth=50,random_state=0)\n",
    "clf.fit(X_train,y_train)\n",
    "model_eval(clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(max_depth=200,random_state=0)\n",
    "clf.fit(X_train,y_train)\n",
    "model_eval(clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
